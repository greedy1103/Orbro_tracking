#!/usr/bin/env python3
"""
OpenPAR Person Attribute Recognition Model
T√≠ch h·ª£p th·ª±c t·∫ø v·ªõi OpenPAR framework t·ª´ Event-AHU
S·ª≠ d·ª•ng pre-trained CLIP models nh∆∞ PromptPAR, SequencePAR, LLM-PAR
H·ªó tr·ª£ ph√¢n lo·∫°i ƒëa thu·ªôc t√≠nh theo chu·∫©n OpenPAR v·ªõi pre-trained weights
"""

import cv2
import numpy as np
import random
from typing import Dict, List, Any, Optional
import json
import os

# Try to import torch, fallback if not available
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import clip # Make CLIP a hard dependency
    HAS_TORCH_AND_CLIP = True
except ImportError:
    HAS_TORCH_AND_CLIP = False
    print("‚ö†Ô∏è PyTorch or CLIP library not found. These are required for the model to run.")

class OpenPARStyleAttributeModel:
    """
    OpenPAR Person Attribute Recognition Model based on CLIP and fine-tuned checkpoints.
    """
    
    def __init__(self, model_type="promptpar", checkpoint_path=None):
        if not HAS_TORCH_AND_CLIP:
            raise ImportError("PyTorch and CLIP must be installed to use this model.")

        print("üöÄ Initializing OpenPAR Person Attribute Model...")
        self.model_type = model_type
        self.checkpoint_path = checkpoint_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # --- START: Major Refactor for Attribute-Specific Prompts ---
        self.attribute_definitions = {
            'Gender': {
                'type': 'exclusive',
                'attributes': {
                    'Male': {'prompts': ['A photo of a person, who appears to be {}', 'This person\'s gender is {}']},
                    'Female': {'prompts': ['A photo of a person, who appears to be {}', 'This person\'s gender is {}']}
                }
            },
            'Age': {
                'type': 'exclusive',
                'attributes': {
                    'Adult': {'prompts': ['An individual who appears to be in the {} age group', 'A photo of an {}']},
                    'Teenager': {'prompts': ['An individual who appears to be in the {} age group', 'A photo of a {}']},
                    'Child': {'prompts': ['An individual who appears to be in the {} age group', 'A photo of a {}']},
                    'Senior': {'prompts': ['An individual who appears to be in the {} age group', 'A photo of a {}']}
                }
            },
            'Body Shape': {
                'type': 'exclusive',
                'attributes': {
                    'Slim': {'prompts': ['This person has a {} build', 'A person with a thin or {} body shape']},
                    'Average': {'prompts': ['This person has an {} build', 'A person with a normal or {} body shape']},
                    'Heavy': {'prompts': ['This person has a {} build', 'A person with an overweight or {} body shape']}
                }
            },
            'Height': {
                'type': 'exclusive',
                'attributes': {
                    'Short': {'prompts': ['This person appears to be {} in height compared to average', 'Relative to their surroundings, this person looks {}']},
                    'Average': {'prompts': ['This person appears to be of {} height', 'Relative to their surroundings, this person looks to be of {} height']},
                    'Tall': {'prompts': ['This person appears to be {} in height compared to average', 'Relative to their surroundings, this person looks {}']}
                }
            },
            'Upper Body': {
                'type': 'exclusive',
                'attributes': {
                    'T-shirt': {'prompts': ['The main shirt this person is wearing is a {}', 'A person wearing a {}']},
                    'Shirt': {'prompts': ['The collared top this person is wearing is a {}', 'A person wearing a {}']},
                    'Jacket': {'prompts': ['The outer layer on their torso is a {}', 'A person wearing a {}']},
                    'Sweater': {'prompts': ['The knitted top this person is wearing is a {}', 'A person wearing a {}']},
                    'Vest': {'prompts': ['A sleeveless top worn over a shirt, which is a {}', 'A person wearing a {}']},
                    'Hoodie': {'prompts': ['A sweatshirt with a hood, which is a {}', 'A person wearing a {}']}
                }
            },
            'Sleeve Length': {
                'type': 'exclusive',
                'attributes': {
                    'Long-sleeved': {'prompts': ['The sleeves on their shirt are {}', 'A top with {} sleeves']},
                    'Short-sleeved': {'prompts': ['The sleeves on their shirt are {}', 'A top with {} sleeves']}
                }
            },
            'Lower Body': {
                'type': 'exclusive',
                'attributes': {
                    'Trousers': {'prompts': ['This person is wearing {} on their legs', 'Their lower body garment is {}']},
                    'Shorts': {'prompts': ['This person is wearing {} on their legs', 'Their lower body garment is {}']},
                    'Skirt': {'prompts': ['This person is wearing a {} on their lower body', 'Their lower body garment is a {}']},
                    'Jeans': {'prompts': ['This person is wearing denim pants, which are {}', 'A pair of {}']}
                }
            },
            'Footwear': {
                'type': 'exclusive',
                'attributes': {
                    'Leather Shoes': {'prompts': ["A photo of this person's formal footwear, which appears to be {}", "They are wearing {}."]},
                    'Sneakers': {'prompts': ["A photo of this person's athletic footwear, which appears to be {}", "They are wearing {}."]},
                    'Boots': {'prompts': ["A photo of this person's high-ankled footwear, which appears to be {}", "They are wearing {}."]}
                }
            },
            'Accessories': {
                'type': 'multi-label',
                'attributes': {
                    'Hat': {'prompts': ['A photo of a person wearing a {} on their head', 'The headwear is a {}']},
                    'Glasses': {'prompts': ['This person is wearing {} on their face', 'A pair of {} on the person']},
                    'Handbag': {'prompts': ['This person is carrying a {} in their hand', 'A separate item, a {}, is being carried']},
                    'Backpack': {'prompts': ['This person is wearing a {} on their back', 'A {} is carried on the shoulders']},
                    'Tie': {'prompts': ['An accessory worn around the neck with a shirt, which is a {}', 'This person is wearing a {}']},
                    'Headphones': {'prompts': ['This person is wearing {} over their ears', 'An audio device, {}, is on the person\'s head']}
                }
            },
            'Hair': {
                'type': 'exclusive',
                'attributes': {
                    'Long Hair': {'prompts': ["A photo of a person with {}.", "This person's hair length is {}"]},
                    'Short Hair': {'prompts': ["A photo of a person with {}.", "This person's hair length is {}"]}
                }
            }
        }
        # --- END: Major Refactor ---
        
        # Initialize models
        self._initialize_models()
        
        print("‚úÖ OpenPAR Model initialized successfully")
        
    def _initialize_models(self):
        """Initialize vision and text encoders from CLIP and load checkpoint."""
        print("‚úÖ Loading base CLIP model (ViT-B/32)...")
        self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
        
        # --- START: T·∫£i Custom Checkpoint ---
        if self.checkpoint_path and os.path.exists(self.checkpoint_path):
            print(f"‚úÖ Found checkpoint: {self.checkpoint_path}. Loading...")
            try:
                checkpoint = torch.load(self.checkpoint_path, map_location=self.device)
                
                # Logic to load state_dict flexibly
                if 'state_dict' in checkpoint:
                    state_dict = checkpoint['state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                else:
                    state_dict = checkpoint

                # Remove unwanted prefixes (e.g., 'module.')
                state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}
                
                self.clip_model.load_state_dict(state_dict, strict=False)
                print("‚úÖ Successfully loaded checkpoint into model.")
            except Exception as e:
                print(f"‚ùå Error loading checkpoint: {e}")
                print("üí° The base CLIP model will be used.")
        else:
            print("‚ö†Ô∏è Checkpoint file not found or path not provided. Using the base CLIP model.")
        # --- END: T·∫£i Custom Checkpoint ---
            
    def predict(self, image, threshold=0.5):
        """
        Predict attributes using the CLIP-based model.
        """
        try:
            # Prepare image
            if isinstance(image, np.ndarray):
                # Convert BGR to RGB
                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                from PIL import Image
                pil_image = Image.fromarray(image_rgb)
                image_tensor = self.clip_preprocess(pil_image).unsqueeze(0).to(self.device)
            else:
                image_tensor = image
                
            with torch.no_grad():
                # Get image features
                image_features = self.clip_model.encode_image(image_tensor)
                image_features = F.normalize(image_features, dim=-1)
                
                # --- START: Logic to handle attribute-specific prompts ---
                all_texts = []
                attribute_map = [] # Stores tuples of (category_name, attribute_name)

                for category_name, category_info in self.attribute_definitions.items():
                    for attribute_name, attribute_info in category_info['attributes'].items():
                        for prompt in attribute_info['prompts']:
                            all_texts.append(prompt.format(attribute_name))
                            attribute_map.append((category_name, attribute_name))

                # Encode text and compute similarity
                text_tokens = clip.tokenize(all_texts).to(self.device)
                text_features = self.clip_model.encode_text(text_tokens)
                text_features = F.normalize(text_features, dim=-1)
                similarities = (image_features @ text_features.T).squeeze(0)

                # Average scores for each attribute (due to multiple prompts)
                final_scores = {}
                attr_prompt_scores = {}
                for i, (_, attr) in enumerate(attribute_map):
                    if attr not in attr_prompt_scores:
                        attr_prompt_scores[attr] = []
                    attr_prompt_scores[attr].append(similarities[i].item())
                
                for attr, scores in attr_prompt_scores.items():
                    final_scores[attr] = np.mean(scores)

                # Process results
                positive_attrs = []
                confidence_scores = {}
                result = {cat_name: [] for cat_name in self.attribute_definitions.keys()}

                for category_name, category_info in self.attribute_definitions.items():
                    category_attributes = list(category_info['attributes'].keys())
                    category_scores = {attr: final_scores.get(attr, -1.0) for attr in category_attributes}
                    
                    # Logic for exclusive categories (choose the best one)
                    if category_info['type'] == 'exclusive':
                        if not category_scores: continue
                        
                        best_attr, best_score = max(category_scores.items(), key=lambda item: item[1])
                        if best_score > threshold:
                            result[category_name].append(best_attr)
                            positive_attrs.append(best_attr)
                            confidence_scores[best_attr] = best_score
                    
                    # Logic for multi-label categories
                    elif category_info['type'] == 'multi-label':
                        for attr, score in category_scores.items():
                            if score > threshold:
                                result[category_name].append(attr)
                                positive_attrs.append(attr)
                                confidence_scores[attr] = score
                
                # --- END: New Logic ---
                
                return {
                    'attributes': result,
                    'positive_attributes': list(set(positive_attrs)), # Ensure no duplicates
                    'confidence_scores': confidence_scores
                }
        except Exception as e:
            print(f"‚ö†Ô∏è An error occurred during attribute prediction: {e}")
            return self._predict_fallback()
            
    def _predict_fallback(self):
        """Fallback prediction with random sampling if a major error occurs."""
        print("Executing fallback prediction due to an error.")
        result = {cat_name: [] for cat_name in self.attribute_definitions.keys()}
        positive_attrs = []
        confidence_scores = {}
        
        # Sample one random attribute
        try:
            random_category = random.choice(list(self.attribute_definitions.values()))
            random_attribute = random.choice(list(random_category['attributes'].keys()))
            positive_attrs.append(random_attribute)
        except:
            pass # Failsafe

        return {
            'attributes': result,
            'positive_attributes': positive_attrs,
            'confidence_scores': confidence_scores
        }
    
    def format_attributes_text(self, attributes_dict):
        """Format attributes as text"""
        text_parts = []
        for category, attrs in attributes_dict.items():
            if attrs:
                attrs_str = ', '.join(attrs)
                text_parts.append(f"{category}: {attrs_str}")
        return '; '.join(text_parts) if text_parts else "No attributes identified"
    
def create_openpar_recognizer(model_type="promptpar", checkpoint_path=None):
    """Factory function to create OpenPAR recognizer."""
    if not HAS_TORCH_AND_CLIP:
        return None
    return OpenPARStyleAttributeModel(model_type=model_type, checkpoint_path=checkpoint_path)

if __name__ == "__main__":
    # Test the model
    recognizer = create_openpar_recognizer()
    
    # Create dummy image
    dummy_image = np.random.randint(0, 255, (256, 128, 3), dtype=np.uint8)
    
    # Test prediction
    result = recognizer.predict(dummy_image)
    
    print("üß™ OpenPAR-Style Test Results:")
    print(f"Detected attributes: {result['positive_attributes']}")
    print(f"Confidence scores: {result['confidence_scores']}")
    print(f"Formatted: {recognizer.format_attributes_text(result['attributes'])}")